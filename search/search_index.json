{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"ml4360/Stereo_Reconstruction/","title":"Preliminaries","text":"<p>How to bring images in a suitable configuration such tha matching is fast? How to obtain depth from the actual measurements(so called disparities)?</p> <p>How to recover 3D from an image? occlusion, parallax, perspective, accomodation, stereopsis\u2026\u2026</p> <p>Why Binocular Stereopsis? - a minimal configuration to percieve depth relatively robustly</p>"},{"location":"ml4360/Stereo_Reconstruction/#two-view-stereo-matching","title":"Two-View Stereo Matching","text":"<p>Goal: Recovering the disparity for every pixel from the input images</p> <p>The disparity defined as the relative displacement between pixels in the two images</p> <p>Task: Construct a dense 3D model from two images of a static scene</p>"},{"location":"ml4360/Stereo_Reconstruction/#pipeline","title":"Pipeline","text":"<p>1.Calibrate cameras intrinsically and extrinsically 2.Rectify images given the calibration 3.Compute disparity map for reference image 4.Remove outliers using consistency/occulation test 5.Obtain depth from disparity using camera calibration 6.Construct 3D model</p>"},{"location":"ml4360/Stereo_Reconstruction/#image-rectification","title":"Image Rectification","text":""},{"location":"ml4360/Structure_from_Motion/","title":"Preliminaries","text":""},{"location":"ml4360/Structure_from_Motion/#camera-calibration","title":"Camera Calibration","text":"<ul> <li>Camera calibration is the process of finding the intrinsic/extrinsic parameters<ul> <li>First, the known calibration target is captured in different poses</li> <li>Second, features on the target are detected in the images</li> <li>Finally,the camera intrinsics and extrinsics are jointly optimized.</li> </ul> </li> </ul>"},{"location":"ml4360/Structure_from_Motion/#feature-detection-and-description","title":"Feature Detection and Description","text":"<ul> <li>Point features describe the appearance of local,salient regionsin an image</li> <li>They can be used to describe and match images taken from different viewpoints.</li> </ul>"},{"location":"ml4360/Structure_from_Motion/#two-frame-sfm","title":"Two-frame SFM","text":""},{"location":"ml4360/Structure_from_Motion/#epipolar-geometry","title":"Epipolar Geometry","text":""},{"location":"ml4360/Structure_from_Motion/#triangulation","title":"Triangulation","text":""},{"location":"ml4360/Structure_from_Motion/#factorization","title":"Factorization","text":""},{"location":"ml4360/Structure_from_Motion/#quesitons","title":"Quesitons?","text":"<ul> <li>I could not fully understand SIFT algorithm</li> </ul>"},{"location":"ml4360/image_formation/","title":"Image formation","text":"<p>key terms:how a 3D scene is projected onto a 2D image plane</p> <p></p>"},{"location":"ml4360/image_formation/#primitives-and-transformations","title":"Primitives and Transformations","text":""},{"location":"ml4360/image_formation/#2d-points","title":"2D Points","text":"<ul> <li>can be wirtten in inhomogeneous coordinates</li> <li>or in homogeneous coordinates</li> <li>projective space</li> </ul> <p>Homogeneous vectors that differ only by scale are considered equivalent and define an equivalence class</p> <ul> <li>An inhomogeneous vector x could be converted to a homogeneous vector -&gt;augmented vector</li> <li>Homogeneous points whose last element is \\(0\\) are called ideal points or points at infinity</li> </ul> <p></p>"},{"location":"ml4360/image_formation/#2d-lines","title":"2D Lines","text":"<ul> <li>using homogeneous coordinates \\(\\widetilde{l}=(a,b,c)^{T}\\)</li> <li>normalize \\(\\tilde{l}\\) </li> <li>line at infinity \\(\\tilde{l}_{\\infty} = (0,0,1)^{T}\\),which passes through all ideal points</li> </ul>"},{"location":"ml4360/image_formation/#cross-product","title":"Cross Product","text":""},{"location":"ml4360/image_formation/#2d-line-arithmetic","title":"2D Line Arithmetic","text":"<ul> <li>intersection of tow lines:\\(\\tilde{x} = \\tilde{l}_1 \\times \\tilde{l}_2\\)</li> <li>line joining two points:\\(\\tilde{l}=\\tilde{x}_1 \\times \\tilde{x}_2\\)</li> </ul>"},{"location":"ml4360/image_formation/#3d-points-and-planes","title":"3D Points and Planes","text":"<ul> <li>the same as case in 2D</li> </ul>"},{"location":"ml4360/image_formation/#3d-lines","title":"3D Lines","text":"<ul> <li>less elegent</li> <li>linear combination of two points</li> </ul>"},{"location":"ml4360/image_formation/#2d-transformations","title":"2D Transformations","text":"<ul> <li>the action of a projective transformation on a co-vector can be represented by the transposed inverse of the matrix</li> </ul>"},{"location":"ml4360/image_formation/#direct-linear-transformation","title":"Direct Linear Transformation","text":"<p>We want a homography estimation using a set of 2D correspondences</p> <ul> <li>Homography estimation:find the relationship between two images of the same scene, but captured from different viewpoints</li> <li></li> </ul> <p>Stacking all equations into a \\(2N \\times 9\\) dimensional matrix \\(A\\) leads to the constrained least squares problem,whose solution is the singular vector corresponding to the smallest singulat value of \\(A\\)(i.e.,the last column of \\(V\\) when decomposing \\(A=UDV^T\\)) derivation</p>"},{"location":"ml4360/image_formation/#geometric-image-formation","title":"Geometric Image Formation","text":""},{"location":"ml4360/image_formation/#orthographic-projection","title":"Orthographic Projection","text":"<ul> <li>The x and y axes of the camera and image coordinate systems are shared</li> <li>Light rats are parallel to the z-coordinate of the camera coordinate system</li> <li>During projection, the z-coordinate is dropped,x and y remain the same  scaled orthorgraphy </li> </ul>"},{"location":"ml4360/image_formation/#perspective-projection","title":"Perspective Projection","text":"<p> - The light ray passes through the camera center,the pixel \\(x_s\\) and the point \\(x_c\\) - Convention:the principal axis aligns with the z-axis - 3D points in camera coordinates are mapped to the image plane by dividing them by their z component and multiplying with thr focal length. - </p> <p>this projection is linear when using homogeneous coordinates</p> <ul> <li>To ensure positive pixel coordinates,a principal point offset is usually added,which moves the image coordinate system to the corner of the image plane. Now we can give the complete perspective projection model as follows: </li> <li>The left \\(3 \\times 3\\) submatrix is called calibration matrix \\(K\\)</li> <li>The parameters of \\(K\\) are called camera intrinsics</li> <li>The skew \\(s\\) arises due to the sensor not mouted perpendicular to the optical axis</li> <li>In practice,we often set \\(f_x =f_y\\) and \\(s=0\\)</li> </ul>"},{"location":"ml4360/image_formation/#chaining-transformations","title":"Chaining Transformations","text":"<p>Full Rank Representation  The homogeneous vector \\(\\tilde{x}_s\\) is a 4D vector and must be normalized wrt. its 3rd entry to obtain inhomogeneous image pixels: \\(\\overline{x}_s = \\tilde{x}_s / z_s = (x_s/z_s,y_s/z_s,1,1/z_s)^{T}\\)</p>"},{"location":"ml4360/image_formation/#photometric-image-formation","title":"Photometric Image Formation","text":"<p>Discuss how an image is formed in terms of pixel intensities and colors - Light is emitted by one or more light sources and reflected of refracted at surfaces of objects in the scene Rendering Equation </p> <p>\\(n^Ts\\)  represents the inner product factor</p> <ul> <li> <p>Typical BRDFs have a diffuse and a specular component</p> <ul> <li>diffuse component scatters light uniformly in all directions</li> <li>specular component depends strongly on the outgoing light direction</li> </ul> <p></p> </li> </ul>"},{"location":"ml4360/image_formation/#why-camera-lenses","title":"Why camera lenses?","text":"<ul> <li>Large and very small pinholes result in image blur</li> <li>small pinholes require long shutter times,which leads to motion blur</li> </ul>"},{"location":"ml4360/image_formation/#thin-lens-model","title":"Thin Lens Model","text":"<p> \\(\\frac{1}{z_s}+\\frac{1}{z_c}=\\frac{1}{f}\\)</p>"},{"location":"ml4360/image_formation/#depth-of-field","title":"Depth of Field","text":"<ul> <li>For \\(z_c \\rightarrow \\infty\\) ,we obtain \\(z_s=f\\)</li> <li>If the image plane is out of foucs,a 3D point projects to the circle of confusion c</li> <li>To control the size of \\(c\\),we change the lens aperture</li> <li>The allowable depth variation that limits the circle of confusion is called DOF and is a function of both the focus distance and the lens aperture <p>distance between the nearest and farthest objects that are acceptably sharp</p> </li> <li>f-number is defined as \\(N = \\frac{f}{d}\\)(\\(d\\): the aperture diameter)</li> </ul>"},{"location":"ml4360/image_formation/#questionsnot-found-in-searching-engine","title":"Questions(Not Found in Searching Engine)","text":"<p>1.normalize \\(\\tilde{l} = (n_x,n_y,-d)^{T}\\) why minus \\(d\\)?</p> <p>2.DLT\uff1f 3.Under orthography,structure and motion can be estimated simultaneously using factorization methods(e.g.,via svm) 4.The skew \\(s\\) arises due to the sensor not mouted perpendicular to the optical axis,only affects x axis?</p>"}]}