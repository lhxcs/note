# Lec6: Training Neural Networks

## Data Preprocessing

假设数据矩阵$X$, 大小为$[N\times D]$, ($N$ 是数据样本的数量，$D$ 是数据的维度)

### 减均值+归一化

首先回顾在激活函数的介绍中，我们提到sigmoid函数的缺点就是不以零为中心，假设神经元的输入全是正的，那么我们梯度下降的优化就会出现zigzag的现象，效率极低。



所以我们考虑对数据中的每个独立特征减去平均值，从几何上理解就是在每个维度上都将数据的中心移到原点。再这之后将数据的所有维度归一化，使其数值范围近似相等。而归一化有两种方法：

- 先对数据做zero-centered处理，然后每个维度除以标准差
- 每个维度都做归一化，使得每个维度最大值和最小值分别为-1和1

![](image/6.1.png)

均值处理的另一个优点就在于，它可以使得线性分类器对于微小的权重变换不那么敏感，使得整个网络能够更好的优化。

![](image/6.2.png)

### PCA and  Whitening

!!! question

    我对PCA过程的理解还不是很透彻，不太理解其中协方差矩阵的具体含义

!!! note

    需要注意的是，任何预处理操作都只能在训练集数据上进行计算，应该先将整个数据集分成训练/验证/测试集，从训练集中求出均值，再在各个集合的图像中减去平均值。



## Weight Initialization

在开始训练神经网络之前，我们还需要初始化神经网络的参数

### 一些错误的idea

- 全零初始化：使得所有神经元都一模一样
- 小随机数初始化：`W=0.01*np.random.randn(D,H)`, 这个方法对于小规模的网络是可取的，但是在大规模的深度网络上，随着层的深入，所有激活值都趋向于零，导致网络不学习。

### 可取的方法

#### Xavier Initalization

使用`1/sqrt(Din)`的方法校准方差。可以保证网络中的神经元有近似的输出分布，提高收敛的速度 (对于卷积层，din是卷积核的维度

!!! note

    该方法的证明将在我学完概率论之后补上

#### Batch Normalization

一个很直观的想法：$\hat{x}^{(k)}=\frac{x^{k}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$

![](image/6.3.png)

